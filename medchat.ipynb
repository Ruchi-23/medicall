{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzaTnxbBnYkkFyXnGk02qR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruchi-23/medicall/blob/main/medchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmTYskE6sHoO"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7QMExepsb5f",
        "outputId": "a86be668-8f29-4a35-e5d9-448663878fe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
      ],
      "metadata": {
        "id": "j1_l1fk2sgfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp7d5OlBsy0T",
        "outputId": "01543ae4-c5d4-4693-c0c3-d3e0cac99750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exmaEydPs5zv",
        "outputId": "02b76275-edcc-497a-84f0-35f832fab553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "JZ2Lc3y1tAMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiT_1EDitCkT",
        "outputId": "6c156aad-3aa5-4194-af2d-a31a6cef46a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "tvbzYfDjtF3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "gZd5_ZDltH2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    \n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "\n",
        "def stem(word):\n",
        "   \n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "\n",
        "def bag_of_words(tokenized_sentence, words):\n",
        "   \n",
        "    # stem each word\n",
        "    sentence_words = [stem(word) for word in tokenized_sentence]\n",
        "    # initialize bag with 0 for each word\n",
        "    bag = np.zeros(len(words), dtype=np.float32)\n",
        "    for idx, w in enumerate(words):\n",
        "        if w in sentence_words: \n",
        "            bag[idx] = 1\n",
        "\n",
        "    return bag"
      ],
      "metadata": {
        "id": "yjjg4XrZtLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "BfLjLmY0tTli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/general_convo.json', 'r') as f:\n",
        "    intents2 = json.load(f)"
      ],
      "metadata": {
        "id": "g5JzqIp-tXRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "tags = []\n",
        "xy = []"
      ],
      "metadata": {
        "id": "Sia8Qa5OtdbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intents2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO3TKcz2thTr",
        "outputId": "b2ec0223-955e-4deb-85dd-655ac4f62c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'tag': 'greeting',\n",
              "   'patterns': ['Hi',\n",
              "    'Hey',\n",
              "    'How are you',\n",
              "    'Is anyone there?',\n",
              "    'Hello',\n",
              "    'Good day'],\n",
              "   'responses': ['Hey :-)',\n",
              "    'Hello, thanks for visiting',\n",
              "    'Hi there, what can I do for you?',\n",
              "    'Hi there, how can I help?']},\n",
              "  {'tag': 'goodbye',\n",
              "   'patterns': ['Bye', 'See you later', 'Goodbye'],\n",
              "   'responses': ['See you later, thanks for visiting',\n",
              "    'Have a nice day',\n",
              "    'Bye! Come back again soon.']},\n",
              "  {'tag': 'thanks',\n",
              "   'patterns': ['Thanks', 'Thank you', \"That's helpful\", \"Thank's a lot!\"],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My pleasure']},\n",
              "  {'tag': 'funny',\n",
              "   'patterns': ['Tell me a joke!',\n",
              "    'Tell me something funny!',\n",
              "    'Do you know a joke?',\n",
              "    \"I don't feel happy\",\n",
              "    'Uplift my mood',\n",
              "    'I feel sad',\n",
              "    'Lift my spirits'],\n",
              "   'responses': [\"Here's a joke for you - Why did the hipster burn his mouth? He drank the coffee before it was cool.\",\n",
              "    \"Here's a joke for you - What did the buffalo say when his son left for college? Bison.\"]},\n",
              "  {'tag': 'Panic disorder',\n",
              "   'patterns': ['Anxiety and nervousness',\n",
              "    \"I'm feeling nervous and anxious\",\n",
              "    \"I'm feeling anxious\",\n",
              "    \"I'm scared\",\n",
              "    'I want to check symptoms for Anxiety, nervousness, dizziness',\n",
              "    'Coughing, sweating, breathless'],\n",
              "   'responses': ['You seem to be having a panic attack. Do not fight it. Stay where you are, if possible. Breathe slowly and deeply. Focus on positive, peaceful and relaxing images.']},\n",
              "  {'tag': 'Cardiac arrest',\n",
              "   'patterns': ['Sharp chest pain',\n",
              "    'Increased heart rate',\n",
              "    'Difficulty in breathing',\n",
              "    'Increased heart rate',\n",
              "    'Call a cardiologist'],\n",
              "   'responses': ['Chew and swallow an aspirin, unless you are allergic to aspirin or have been told by your doctor never to take aspirin. Cardiologists nearby : https://bit.ly/cardiologists_nearby , A hospital near you : http://pulsecardiologygroup.co.uk/sussex-cardiovascular-clinic/']},\n",
              "  {'tag': 'Glaucoma',\n",
              "   'patterns': ['Do I have glaucoma?',\n",
              "    'Diminished vision',\n",
              "    'Pain in the eye',\n",
              "    'I see spots or clouds in vision',\n",
              "    'spots or clouds in vision'],\n",
              "   'responses': ['Consult an opthalmologist nearby. Follow this link to book an appointment - https://bit.ly/opthalmologists_nearby']},\n",
              "  {'tag': 'Eating disorder',\n",
              "   'patterns': ['Depression, anxiety and nervousness, depressive or psychotic symptoms',\n",
              "    'Decreased appetite',\n",
              "    'Abusing alcohol',\n",
              "    'Excessive eating'],\n",
              "   'responses': ['Eating disorder treatment depends on your particular disorder and your symptoms. It typically includes a combination of psychological therapy (psychotherapy), nutrition education, medical monitoring and sometimes medications. Search for a mental health professional or a registered dietician nearby - https://bit.ly/psychologists_nearby']},\n",
              "  {'tag': 'Liver cancer',\n",
              "   'patterns': ['sharp abdominal pain',\n",
              "    'upper abdominal pain',\n",
              "    'changes in the appearance of stool',\n",
              "    'Decreased appetite',\n",
              "    'unusual color or odour to urine',\n",
              "    'painful urination'],\n",
              "   'responses': ['Consult a hepatologist or a urologist nearby - https://bit.ly/urologists_nearby']},\n",
              "  {'tag': 'Asthma',\n",
              "   'patterns': ['coughing,wheezing',\n",
              "    'chest tightness',\n",
              "    'trouble sleeping due to shortness of breath',\n",
              "    'breathlessness',\n",
              "    \"can't sleep due to breathlessness\"],\n",
              "   'responses': ['Find best pulmonologists nearby. Book an appointment now - https://bit.ly/pulmonologists_nearby']},\n",
              "  {'tag': 'Dengue fever',\n",
              "   'patterns': ['I have a very high fever',\n",
              "    'high fever',\n",
              "    'why do I have very high fever?',\n",
              "    'Very high body temperature',\n",
              "    'lower platelet count'],\n",
              "   'responses': ['Have a paracetamol to reduce the temperature. Drink lots of water, keep yourself hydrated. Find the best general physicians nearby - https://bit.ly/pcp_nearby']},\n",
              "  {'tag': 'Obesity',\n",
              "   'patterns': [\"How do I know if I'm obese?\",\n",
              "    'Am I obsese?',\n",
              "    'BMI calculator',\n",
              "    'Get rid of obesity',\n",
              "    'How do I get rid of obesity?'],\n",
              "   'responses': [\"Check your BMI here to know if you're really obese- https://patient.info/doctor/bmi-calculator-calculator. If you're obese or extremely obese, consider consulting a dietician nearby. Find dieticians/nutritionists nearby - https://bit.ly/diet_care\"]},\n",
              "  {'tag': 'Vitamin deficiency',\n",
              "   'patterns': ['pale skin',\n",
              "    'muscle weakness',\n",
              "    'I feel very sleepy',\n",
              "    'I feel exhausted',\n",
              "    'tired',\n",
              "    'Forgetfulness'],\n",
              "   'responses': ['Eat healthy and organic foods. Practise yoga everyday. Include fruits and green vegetables in your diet. Consult a dietician for advice - https://bit.ly/diet_care ']},\n",
              "  {'tag': 'Diabetes',\n",
              "   'patterns': ['Frequent urination',\n",
              "    'why do I pee after every 10 minutes?',\n",
              "    'Why do I feel so hungry?',\n",
              "    'feel very tired',\n",
              "    'sudden weight loss'],\n",
              "   'responses': ['Take care of your diet. Consult a dietician/nutritionist nearby - https://bit.ly/diet_care . Also, consider getting yourself tested for diabetes soon. ']},\n",
              "  {'tag': 'Loss of apetite',\n",
              "   'patterns': ['Loss of apetite',\n",
              "    \"I don't feel like eating\",\n",
              "    'unable to eat',\n",
              "    'less apetite',\n",
              "    'my apetite has decreased recently'],\n",
              "   'responses': ['Eat small meals regularly throughout the day. Talk to a nutritionist about your eating habits if they’re irregular - https://bit.ly/diet_care']},\n",
              "  {'tag': 'Fatty liver',\n",
              "   'patterns': ['I have been diagnosed with fatty liver',\n",
              "    'I have fatty liver',\n",
              "    'An increased build-up of fat in the liver',\n",
              "    'increased levels of aminotransferase (ALT) and aspartate aminotransferase (AST)'],\n",
              "   'responses': ['To treat it at home, include more fruits and vegetables in your diet',\n",
              "    'significantly reducing intake of certain foods and beverages including those high in added sugar, salt, refined carbohydrates, and saturated fat will help',\n",
              "    'Your daily cup of coffee could help protect your liver against NAFLD.']},\n",
              "  {'tag': 'Joint pain',\n",
              "   'patterns': ['joint pain',\n",
              "    'I have pain in my joints',\n",
              "    'body ache',\n",
              "    'joint pain'],\n",
              "   'responses': ['For moderate-to-severe joint pain with swelling, an over-the-counter or prescription nonsteroidal anti-inflammatory drug (NSAID) such as aspirin, celecoxib, ibuprofen, or naproxen can provide relief. NSAIDs can have side effects, potentially increasing your risk for gastrointestinal bleeding.If you have mild pain without any swelling, acetaminophen can be effective. Be careful when taking this medicine though, especially if you drink alcohol, because high doses may cause liver damage. Because of the risks, you should take any of these pain medications with caution.']},\n",
              "  {'tag': 'Tooth ache',\n",
              "   'patterns': ['Tooth ache',\n",
              "    'tooth decay',\n",
              "    'cavity',\n",
              "    'bad breath',\n",
              "    'mouth smell',\n",
              "    'tooth cavity'],\n",
              "   'responses': ['Salt water gargles will help soothe the pain. Book an appointment with a nearby dentist - https://bit.ly/dentists_visit']},\n",
              "  {'tag': 'Find doctors',\n",
              "   'patterns': ['find doctors',\n",
              "    'find clinics',\n",
              "    'search doctors',\n",
              "    'search clinics',\n",
              "    'look up for doctors',\n",
              "    'search hospitals'],\n",
              "   'responses': ['Websites to find nearest hospitals include : Zocdoc - https://www.zocdoc.com/ , Healthgrades - https://www.healthgrades.com/ , WebMD - https://www.webmd.com/ , Yelp - https://www.yelp.com/ , Google Maps - https://www.google.com/maps/']},\n",
              "  {'tag': 'diagnostic centres',\n",
              "   'patterns': ['diagnostic centres', 'test centres', 'check up'],\n",
              "   'responses': ['Websites to find diagnostic centres nearby : BMI Healthcare - https://www.bmihealthcare.co.uk/ , Nuffield Health - https://www.nuffieldhealth.com/ , The London Clinic - https://www.thelondonclinic.co.uk/ , Spire Healthcare - https://www.spirehealthcare.com/ , Medicspot - https://www.medicspot.co.uk/']}]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in intents2['intents']:\n",
        "    tag = intent['tag']\n",
        "    # add to tag list\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = tokenize(pattern)\n",
        "        # add to our words list\n",
        "        all_words.extend(w)\n",
        "        # add to xy pair\n",
        "        xy.append((w, tag))\n",
        "ignore_words = ['?', '.', '!',' ','+','-','%',';','`','\"\"','/','°', '’',\"'\", '(', ')', ',',':', '``']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "# remove duplicates and sort\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(len(xy), \"patterns\")\n",
        "print(len(tags), \"tags:\", tags)\n",
        "print(len(all_words), \"unique stemmed words:\", all_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUGTX1zrtkVC",
        "outputId": "4457c625-56d0-48d0-f51a-e160dc84393b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 patterns\n",
            "20 tags: ['Asthma', 'Cardiac arrest', 'Dengue fever', 'Diabetes', 'Eating disorder', 'Fatty liver', 'Find doctors', 'Glaucoma', 'Joint pain', 'Liver cancer', 'Loss of apetite', 'Obesity', 'Panic disorder', 'Tooth ache', 'Vitamin deficiency', 'diagnostic centres', 'funny', 'goodbye', 'greeting', 'thanks']\n",
            "172 unique stemmed words: [\"'m\", \"'s\", '10', 'a', 'abdomin', 'abus', 'ach', 'after', 'alcohol', 'alt', 'am', 'aminotransferas', 'an', 'and', 'anxieti', 'anxiou', 'anyon', 'apetit', 'appear', 'appetit', 'are', 'aspart', 'ast', 'bad', 'been', 'bmi', 'bodi', 'breath', 'breathless', 'build-up', 'bye', 'ca', 'calcul', 'call', 'cardiologist', 'caviti', 'centr', 'chang', 'check', 'chest', 'clinic', 'cloud', 'color', 'cough', 'count', 'day', 'decay', 'decreas', 'depress', 'diagnos', 'diagnost', 'difficulti', 'diminish', 'dizzi', 'do', 'doctor', 'due', 'eat', 'everi', 'excess', 'exhaust', 'eye', 'fat', 'fatti', 'feel', 'fever', 'find', 'for', 'forget', 'frequent', 'funni', 'get', 'glaucoma', 'good', 'goodby', 'ha', 'happi', 'have', 'heart', 'hello', 'help', 'hey', 'hi', 'high', 'hospit', 'how', 'hungri', 'i', 'if', 'in', 'increas', 'is', 'joint', 'joke', 'know', 'later', 'less', 'level', 'lift', 'like', 'liver', 'look', 'loss', 'lot', 'lower', 'me', 'minut', 'mood', 'mouth', 'muscl', 'my', \"n't\", 'nervou', 'nervous', 'obes', 'obses', 'odour', 'of', 'or', 'pain', 'pale', 'pee', 'platelet', 'psychot', 'rate', 'recent', 'rid', 'sad', 'scare', 'search', 'see', 'sharp', 'short', 'skin', 'sleep', 'sleepi', 'smell', 'so', 'someth', 'spirit', 'spot', 'stool', 'sudden', 'sweat', 'symptom', 'tell', 'temperatur', 'test', 'thank', 'that', 'the', 'there', 'tight', 'tire', 'to', 'tooth', 'troubl', 'unabl', 'unusu', 'up', 'uplift', 'upper', 'urin', 'veri', 'vision', 'want', 'weak', 'weight', 'wheez', 'whi', 'with', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    # X: bag of words for each pattern_sentence\n",
        "    bag = bag_of_words(pattern_sentence, all_words)\n",
        "    X_train.append(bag)\n",
        "    # y: PyTorch CrossEntropyLoss needs only class labels, not one-hot\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "Zf7yvJ0utqZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.l1 = nn.Linear(input_size, hidden_size) \n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size) \n",
        "        self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # no activation and no softmax at the end\n",
        "        return out"
      ],
      "metadata": {
        "id": "OwxfY1EdtwM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 500\n",
        "batch_size = 8\n",
        "learning_rate = 0.001\n",
        "input_size = len(X_train[0])\n",
        "hidden_size = 8\n",
        "output_size = len(tags)\n",
        "print(input_size, output_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUCUpSEKtxUu",
        "outputId": "146db6af-4be6-4b4c-8008-5c05c632ac31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.n_samples = len(X_train)\n",
        "        self.x_data = X_train\n",
        "        self.y_data = y_train\n",
        "\n",
        "    # support indexing such that dataset[i] can be used to get i-th sample\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    # we can call len(dataset) to return the size\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "bUIHt9p6tzxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ChatDataset()"
      ],
      "metadata": {
        "id": "XEkyQhgpt5Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "T9UsXH8Jt8Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for (words, labels) in train_loader:\n",
        "        words = words.to(device)\n",
        "        labels = labels.to(dtype=torch.long).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(words)\n",
        "        # if y would be one-hot, we must apply\n",
        "        # labels = torch.max(labels, 1)[1]\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    print (f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "print(f'final loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_dZYRA4t_yb",
        "outputId": "1ed7f89f-a9b6-4ae0-85b6-85a2d2c0e2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 3.0116\n",
            "Epoch [2/500], Loss: 3.0984\n",
            "Epoch [3/500], Loss: 3.1422\n",
            "Epoch [4/500], Loss: 3.0991\n",
            "Epoch [5/500], Loss: 3.0134\n",
            "Epoch [6/500], Loss: 3.0933\n",
            "Epoch [7/500], Loss: 3.1426\n",
            "Epoch [8/500], Loss: 2.9228\n",
            "Epoch [9/500], Loss: 2.8739\n",
            "Epoch [10/500], Loss: 3.0537\n",
            "Epoch [11/500], Loss: 2.9659\n",
            "Epoch [12/500], Loss: 2.9038\n",
            "Epoch [13/500], Loss: 2.9828\n",
            "Epoch [14/500], Loss: 3.1100\n",
            "Epoch [15/500], Loss: 2.9222\n",
            "Epoch [16/500], Loss: 2.9025\n",
            "Epoch [17/500], Loss: 2.9595\n",
            "Epoch [18/500], Loss: 2.7180\n",
            "Epoch [19/500], Loss: 3.0609\n",
            "Epoch [20/500], Loss: 2.7095\n",
            "Epoch [21/500], Loss: 2.6809\n",
            "Epoch [22/500], Loss: 2.4892\n",
            "Epoch [23/500], Loss: 2.5854\n",
            "Epoch [24/500], Loss: 2.7737\n",
            "Epoch [25/500], Loss: 2.8511\n",
            "Epoch [26/500], Loss: 2.9816\n",
            "Epoch [27/500], Loss: 2.9919\n",
            "Epoch [28/500], Loss: 3.0279\n",
            "Epoch [29/500], Loss: 2.9418\n",
            "Epoch [30/500], Loss: 2.7834\n",
            "Epoch [31/500], Loss: 2.2192\n",
            "Epoch [32/500], Loss: 2.5383\n",
            "Epoch [33/500], Loss: 2.5750\n",
            "Epoch [34/500], Loss: 2.6474\n",
            "Epoch [35/500], Loss: 2.7397\n",
            "Epoch [36/500], Loss: 2.6733\n",
            "Epoch [37/500], Loss: 2.4642\n",
            "Epoch [38/500], Loss: 2.5350\n",
            "Epoch [39/500], Loss: 2.7243\n",
            "Epoch [40/500], Loss: 1.8951\n",
            "Epoch [41/500], Loss: 2.3810\n",
            "Epoch [42/500], Loss: 2.1157\n",
            "Epoch [43/500], Loss: 2.1415\n",
            "Epoch [44/500], Loss: 1.4495\n",
            "Epoch [45/500], Loss: 2.4442\n",
            "Epoch [46/500], Loss: 2.1637\n",
            "Epoch [47/500], Loss: 2.7099\n",
            "Epoch [48/500], Loss: 2.3795\n",
            "Epoch [49/500], Loss: 1.9254\n",
            "Epoch [50/500], Loss: 2.2030\n",
            "Epoch [51/500], Loss: 1.7106\n",
            "Epoch [52/500], Loss: 1.7267\n",
            "Epoch [53/500], Loss: 1.4806\n",
            "Epoch [54/500], Loss: 1.2729\n",
            "Epoch [55/500], Loss: 1.3407\n",
            "Epoch [56/500], Loss: 1.3198\n",
            "Epoch [57/500], Loss: 1.9774\n",
            "Epoch [58/500], Loss: 1.8688\n",
            "Epoch [59/500], Loss: 1.9233\n",
            "Epoch [60/500], Loss: 1.6088\n",
            "Epoch [61/500], Loss: 1.8000\n",
            "Epoch [62/500], Loss: 1.4841\n",
            "Epoch [63/500], Loss: 1.2696\n",
            "Epoch [64/500], Loss: 1.0207\n",
            "Epoch [65/500], Loss: 1.3759\n",
            "Epoch [66/500], Loss: 1.4155\n",
            "Epoch [67/500], Loss: 0.8040\n",
            "Epoch [68/500], Loss: 1.0019\n",
            "Epoch [69/500], Loss: 1.7749\n",
            "Epoch [70/500], Loss: 1.4327\n",
            "Epoch [71/500], Loss: 1.5684\n",
            "Epoch [72/500], Loss: 0.8931\n",
            "Epoch [73/500], Loss: 1.0059\n",
            "Epoch [74/500], Loss: 0.9494\n",
            "Epoch [75/500], Loss: 1.9853\n",
            "Epoch [76/500], Loss: 1.9500\n",
            "Epoch [77/500], Loss: 0.6146\n",
            "Epoch [78/500], Loss: 1.2964\n",
            "Epoch [79/500], Loss: 0.5506\n",
            "Epoch [80/500], Loss: 0.7838\n",
            "Epoch [81/500], Loss: 1.4046\n",
            "Epoch [82/500], Loss: 1.5858\n",
            "Epoch [83/500], Loss: 1.3210\n",
            "Epoch [84/500], Loss: 0.4138\n",
            "Epoch [85/500], Loss: 0.9836\n",
            "Epoch [86/500], Loss: 0.4689\n",
            "Epoch [87/500], Loss: 0.2016\n",
            "Epoch [88/500], Loss: 0.8743\n",
            "Epoch [89/500], Loss: 1.4490\n",
            "Epoch [90/500], Loss: 1.3275\n",
            "Epoch [91/500], Loss: 0.6415\n",
            "Epoch [92/500], Loss: 0.8380\n",
            "Epoch [93/500], Loss: 1.1454\n",
            "Epoch [94/500], Loss: 1.1564\n",
            "Epoch [95/500], Loss: 0.5995\n",
            "Epoch [96/500], Loss: 0.6026\n",
            "Epoch [97/500], Loss: 0.9254\n",
            "Epoch [98/500], Loss: 0.6530\n",
            "Epoch [99/500], Loss: 0.3461\n",
            "Epoch [100/500], Loss: 0.8853\n",
            "Epoch [101/500], Loss: 0.5185\n",
            "Epoch [102/500], Loss: 1.2221\n",
            "Epoch [103/500], Loss: 0.6707\n",
            "Epoch [104/500], Loss: 0.6114\n",
            "Epoch [105/500], Loss: 0.8698\n",
            "Epoch [106/500], Loss: 0.9201\n",
            "Epoch [107/500], Loss: 0.2677\n",
            "Epoch [108/500], Loss: 0.2139\n",
            "Epoch [109/500], Loss: 1.0316\n",
            "Epoch [110/500], Loss: 0.7290\n",
            "Epoch [111/500], Loss: 0.5304\n",
            "Epoch [112/500], Loss: 0.3853\n",
            "Epoch [113/500], Loss: 0.3732\n",
            "Epoch [114/500], Loss: 0.5513\n",
            "Epoch [115/500], Loss: 0.1800\n",
            "Epoch [116/500], Loss: 0.5069\n",
            "Epoch [117/500], Loss: 0.2651\n",
            "Epoch [118/500], Loss: 0.8183\n",
            "Epoch [119/500], Loss: 0.3659\n",
            "Epoch [120/500], Loss: 0.4667\n",
            "Epoch [121/500], Loss: 0.3961\n",
            "Epoch [122/500], Loss: 0.8365\n",
            "Epoch [123/500], Loss: 0.2830\n",
            "Epoch [124/500], Loss: 0.2130\n",
            "Epoch [125/500], Loss: 0.6271\n",
            "Epoch [126/500], Loss: 0.3269\n",
            "Epoch [127/500], Loss: 0.1861\n",
            "Epoch [128/500], Loss: 0.3922\n",
            "Epoch [129/500], Loss: 0.4208\n",
            "Epoch [130/500], Loss: 0.5565\n",
            "Epoch [131/500], Loss: 0.4743\n",
            "Epoch [132/500], Loss: 0.0976\n",
            "Epoch [133/500], Loss: 0.2036\n",
            "Epoch [134/500], Loss: 0.0890\n",
            "Epoch [135/500], Loss: 0.0984\n",
            "Epoch [136/500], Loss: 0.3737\n",
            "Epoch [137/500], Loss: 0.1571\n",
            "Epoch [138/500], Loss: 0.1491\n",
            "Epoch [139/500], Loss: 0.4562\n",
            "Epoch [140/500], Loss: 0.2392\n",
            "Epoch [141/500], Loss: 0.2562\n",
            "Epoch [142/500], Loss: 0.3037\n",
            "Epoch [143/500], Loss: 0.1949\n",
            "Epoch [144/500], Loss: 0.2644\n",
            "Epoch [145/500], Loss: 0.1673\n",
            "Epoch [146/500], Loss: 0.1246\n",
            "Epoch [147/500], Loss: 0.2007\n",
            "Epoch [148/500], Loss: 0.0973\n",
            "Epoch [149/500], Loss: 0.4609\n",
            "Epoch [150/500], Loss: 0.1204\n",
            "Epoch [151/500], Loss: 0.1102\n",
            "Epoch [152/500], Loss: 0.0837\n",
            "Epoch [153/500], Loss: 0.0198\n",
            "Epoch [154/500], Loss: 0.3357\n",
            "Epoch [155/500], Loss: 0.3295\n",
            "Epoch [156/500], Loss: 0.2993\n",
            "Epoch [157/500], Loss: 0.2104\n",
            "Epoch [158/500], Loss: 0.4668\n",
            "Epoch [159/500], Loss: 0.1235\n",
            "Epoch [160/500], Loss: 0.3909\n",
            "Epoch [161/500], Loss: 0.2972\n",
            "Epoch [162/500], Loss: 0.0812\n",
            "Epoch [163/500], Loss: 0.0387\n",
            "Epoch [164/500], Loss: 0.2013\n",
            "Epoch [165/500], Loss: 0.1165\n",
            "Epoch [166/500], Loss: 0.1484\n",
            "Epoch [167/500], Loss: 0.0181\n",
            "Epoch [168/500], Loss: 0.2510\n",
            "Epoch [169/500], Loss: 0.0121\n",
            "Epoch [170/500], Loss: 0.1283\n",
            "Epoch [171/500], Loss: 0.1189\n",
            "Epoch [172/500], Loss: 0.1211\n",
            "Epoch [173/500], Loss: 0.2071\n",
            "Epoch [174/500], Loss: 0.2138\n",
            "Epoch [175/500], Loss: 0.0528\n",
            "Epoch [176/500], Loss: 0.2504\n",
            "Epoch [177/500], Loss: 0.0826\n",
            "Epoch [178/500], Loss: 0.0704\n",
            "Epoch [179/500], Loss: 0.0503\n",
            "Epoch [180/500], Loss: 0.2637\n",
            "Epoch [181/500], Loss: 0.0776\n",
            "Epoch [182/500], Loss: 0.0500\n",
            "Epoch [183/500], Loss: 0.0528\n",
            "Epoch [184/500], Loss: 0.1068\n",
            "Epoch [185/500], Loss: 0.2235\n",
            "Epoch [186/500], Loss: 0.3120\n",
            "Epoch [187/500], Loss: 0.0338\n",
            "Epoch [188/500], Loss: 0.1404\n",
            "Epoch [189/500], Loss: 0.0730\n",
            "Epoch [190/500], Loss: 0.0869\n",
            "Epoch [191/500], Loss: 0.1715\n",
            "Epoch [192/500], Loss: 0.0529\n",
            "Epoch [193/500], Loss: 0.0275\n",
            "Epoch [194/500], Loss: 0.0063\n",
            "Epoch [195/500], Loss: 0.0846\n",
            "Epoch [196/500], Loss: 0.0289\n",
            "Epoch [197/500], Loss: 0.0502\n",
            "Epoch [198/500], Loss: 0.0846\n",
            "Epoch [199/500], Loss: 0.0876\n",
            "Epoch [200/500], Loss: 0.0713\n",
            "Epoch [201/500], Loss: 0.0920\n",
            "Epoch [202/500], Loss: 0.1052\n",
            "Epoch [203/500], Loss: 0.0691\n",
            "Epoch [204/500], Loss: 0.0606\n",
            "Epoch [205/500], Loss: 0.0326\n",
            "Epoch [206/500], Loss: 0.0159\n",
            "Epoch [207/500], Loss: 0.0383\n",
            "Epoch [208/500], Loss: 0.0227\n",
            "Epoch [209/500], Loss: 0.0376\n",
            "Epoch [210/500], Loss: 0.0553\n",
            "Epoch [211/500], Loss: 0.0700\n",
            "Epoch [212/500], Loss: 0.0157\n",
            "Epoch [213/500], Loss: 0.0266\n",
            "Epoch [214/500], Loss: 0.0536\n",
            "Epoch [215/500], Loss: 0.0508\n",
            "Epoch [216/500], Loss: 0.0590\n",
            "Epoch [217/500], Loss: 0.0243\n",
            "Epoch [218/500], Loss: 0.0187\n",
            "Epoch [219/500], Loss: 0.0119\n",
            "Epoch [220/500], Loss: 0.0634\n",
            "Epoch [221/500], Loss: 0.0065\n",
            "Epoch [222/500], Loss: 0.0215\n",
            "Epoch [223/500], Loss: 0.0373\n",
            "Epoch [224/500], Loss: 0.0733\n",
            "Epoch [225/500], Loss: 0.0239\n",
            "Epoch [226/500], Loss: 0.0264\n",
            "Epoch [227/500], Loss: 0.0717\n",
            "Epoch [228/500], Loss: 0.0383\n",
            "Epoch [229/500], Loss: 0.0425\n",
            "Epoch [230/500], Loss: 0.0364\n",
            "Epoch [231/500], Loss: 0.0314\n",
            "Epoch [232/500], Loss: 0.0237\n",
            "Epoch [233/500], Loss: 0.0531\n",
            "Epoch [234/500], Loss: 0.0363\n",
            "Epoch [235/500], Loss: 0.0128\n",
            "Epoch [236/500], Loss: 0.0257\n",
            "Epoch [237/500], Loss: 0.0479\n",
            "Epoch [238/500], Loss: 0.0254\n",
            "Epoch [239/500], Loss: 0.0136\n",
            "Epoch [240/500], Loss: 0.0329\n",
            "Epoch [241/500], Loss: 0.0425\n",
            "Epoch [242/500], Loss: 0.0163\n",
            "Epoch [243/500], Loss: 0.0263\n",
            "Epoch [244/500], Loss: 0.0451\n",
            "Epoch [245/500], Loss: 0.0240\n",
            "Epoch [246/500], Loss: 0.0306\n",
            "Epoch [247/500], Loss: 0.0535\n",
            "Epoch [248/500], Loss: 0.0228\n",
            "Epoch [249/500], Loss: 0.0984\n",
            "Epoch [250/500], Loss: 0.0240\n",
            "Epoch [251/500], Loss: 0.0278\n",
            "Epoch [252/500], Loss: 0.0339\n",
            "Epoch [253/500], Loss: 0.0182\n",
            "Epoch [254/500], Loss: 0.0084\n",
            "Epoch [255/500], Loss: 0.0132\n",
            "Epoch [256/500], Loss: 0.0413\n",
            "Epoch [257/500], Loss: 0.0097\n",
            "Epoch [258/500], Loss: 0.0198\n",
            "Epoch [259/500], Loss: 0.1896\n",
            "Epoch [260/500], Loss: 0.0288\n",
            "Epoch [261/500], Loss: 0.1830\n",
            "Epoch [262/500], Loss: 0.0066\n",
            "Epoch [263/500], Loss: 0.0154\n",
            "Epoch [264/500], Loss: 0.0341\n",
            "Epoch [265/500], Loss: 0.0168\n",
            "Epoch [266/500], Loss: 0.0037\n",
            "Epoch [267/500], Loss: 0.2258\n",
            "Epoch [268/500], Loss: 0.0443\n",
            "Epoch [269/500], Loss: 0.0152\n",
            "Epoch [270/500], Loss: 0.0091\n",
            "Epoch [271/500], Loss: 0.0287\n",
            "Epoch [272/500], Loss: 0.0092\n",
            "Epoch [273/500], Loss: 0.0040\n",
            "Epoch [274/500], Loss: 0.0097\n",
            "Epoch [275/500], Loss: 0.0120\n",
            "Epoch [276/500], Loss: 0.0033\n",
            "Epoch [277/500], Loss: 0.0124\n",
            "Epoch [278/500], Loss: 0.0079\n",
            "Epoch [279/500], Loss: 0.0058\n",
            "Epoch [280/500], Loss: 0.0119\n",
            "Epoch [281/500], Loss: 0.0114\n",
            "Epoch [282/500], Loss: 0.0464\n",
            "Epoch [283/500], Loss: 0.0226\n",
            "Epoch [284/500], Loss: 0.0040\n",
            "Epoch [285/500], Loss: 0.0020\n",
            "Epoch [286/500], Loss: 0.0213\n",
            "Epoch [287/500], Loss: 0.0120\n",
            "Epoch [288/500], Loss: 0.0059\n",
            "Epoch [289/500], Loss: 0.0115\n",
            "Epoch [290/500], Loss: 0.0083\n",
            "Epoch [291/500], Loss: 0.0108\n",
            "Epoch [292/500], Loss: 0.0121\n",
            "Epoch [293/500], Loss: 0.0150\n",
            "Epoch [294/500], Loss: 0.0096\n",
            "Epoch [295/500], Loss: 0.0076\n",
            "Epoch [296/500], Loss: 0.0135\n",
            "Epoch [297/500], Loss: 0.0126\n",
            "Epoch [298/500], Loss: 0.0126\n",
            "Epoch [299/500], Loss: 0.0064\n",
            "Epoch [300/500], Loss: 0.0129\n",
            "Epoch [301/500], Loss: 0.2034\n",
            "Epoch [302/500], Loss: 0.1945\n",
            "Epoch [303/500], Loss: 0.0100\n",
            "Epoch [304/500], Loss: 0.1384\n",
            "Epoch [305/500], Loss: 0.0058\n",
            "Epoch [306/500], Loss: 0.1300\n",
            "Epoch [307/500], Loss: 0.0095\n",
            "Epoch [308/500], Loss: 0.0040\n",
            "Epoch [309/500], Loss: 0.0142\n",
            "Epoch [310/500], Loss: 0.0078\n",
            "Epoch [311/500], Loss: 0.0105\n",
            "Epoch [312/500], Loss: 0.0072\n",
            "Epoch [313/500], Loss: 0.0110\n",
            "Epoch [314/500], Loss: 0.0101\n",
            "Epoch [315/500], Loss: 0.0265\n",
            "Epoch [316/500], Loss: 0.0012\n",
            "Epoch [317/500], Loss: 0.0267\n",
            "Epoch [318/500], Loss: 0.0078\n",
            "Epoch [319/500], Loss: 0.0042\n",
            "Epoch [320/500], Loss: 0.0096\n",
            "Epoch [321/500], Loss: 0.0046\n",
            "Epoch [322/500], Loss: 0.0064\n",
            "Epoch [323/500], Loss: 0.0077\n",
            "Epoch [324/500], Loss: 0.0260\n",
            "Epoch [325/500], Loss: 0.1947\n",
            "Epoch [326/500], Loss: 0.0075\n",
            "Epoch [327/500], Loss: 0.0086\n",
            "Epoch [328/500], Loss: 0.0185\n",
            "Epoch [329/500], Loss: 0.0080\n",
            "Epoch [330/500], Loss: 0.0041\n",
            "Epoch [331/500], Loss: 0.0139\n",
            "Epoch [332/500], Loss: 0.0126\n",
            "Epoch [333/500], Loss: 0.0047\n",
            "Epoch [334/500], Loss: 0.0040\n",
            "Epoch [335/500], Loss: 0.0058\n",
            "Epoch [336/500], Loss: 0.0025\n",
            "Epoch [337/500], Loss: 0.0009\n",
            "Epoch [338/500], Loss: 0.0071\n",
            "Epoch [339/500], Loss: 0.0051\n",
            "Epoch [340/500], Loss: 0.2152\n",
            "Epoch [341/500], Loss: 0.0052\n",
            "Epoch [342/500], Loss: 0.0049\n",
            "Epoch [343/500], Loss: 0.0011\n",
            "Epoch [344/500], Loss: 0.0118\n",
            "Epoch [345/500], Loss: 0.0198\n",
            "Epoch [346/500], Loss: 0.0208\n",
            "Epoch [347/500], Loss: 0.0055\n",
            "Epoch [348/500], Loss: 0.0034\n",
            "Epoch [349/500], Loss: 0.0054\n",
            "Epoch [350/500], Loss: 0.0005\n",
            "Epoch [351/500], Loss: 0.0080\n",
            "Epoch [352/500], Loss: 0.0043\n",
            "Epoch [353/500], Loss: 0.0100\n",
            "Epoch [354/500], Loss: 0.0030\n",
            "Epoch [355/500], Loss: 0.0021\n",
            "Epoch [356/500], Loss: 0.0088\n",
            "Epoch [357/500], Loss: 0.0057\n",
            "Epoch [358/500], Loss: 0.3500\n",
            "Epoch [359/500], Loss: 0.0123\n",
            "Epoch [360/500], Loss: 0.1958\n",
            "Epoch [361/500], Loss: 0.0033\n",
            "Epoch [362/500], Loss: 0.0026\n",
            "Epoch [363/500], Loss: 0.0037\n",
            "Epoch [364/500], Loss: 0.0165\n",
            "Epoch [365/500], Loss: 0.0062\n",
            "Epoch [366/500], Loss: 0.0037\n",
            "Epoch [367/500], Loss: 0.0052\n",
            "Epoch [368/500], Loss: 0.0029\n",
            "Epoch [369/500], Loss: 0.0018\n",
            "Epoch [370/500], Loss: 0.0014\n",
            "Epoch [371/500], Loss: 0.0024\n",
            "Epoch [372/500], Loss: 0.0067\n",
            "Epoch [373/500], Loss: 0.0038\n",
            "Epoch [374/500], Loss: 0.0082\n",
            "Epoch [375/500], Loss: 0.0074\n",
            "Epoch [376/500], Loss: 0.0058\n",
            "Epoch [377/500], Loss: 0.0079\n",
            "Epoch [378/500], Loss: 0.0023\n",
            "Epoch [379/500], Loss: 0.2177\n",
            "Epoch [380/500], Loss: 0.0035\n",
            "Epoch [381/500], Loss: 0.0090\n",
            "Epoch [382/500], Loss: 0.0028\n",
            "Epoch [383/500], Loss: 0.0071\n",
            "Epoch [384/500], Loss: 0.0033\n",
            "Epoch [385/500], Loss: 0.0050\n",
            "Epoch [386/500], Loss: 0.0073\n",
            "Epoch [387/500], Loss: 0.0009\n",
            "Epoch [388/500], Loss: 0.0036\n",
            "Epoch [389/500], Loss: 0.2129\n",
            "Epoch [390/500], Loss: 0.0041\n",
            "Epoch [391/500], Loss: 0.0029\n",
            "Epoch [392/500], Loss: 0.0051\n",
            "Epoch [393/500], Loss: 0.0056\n",
            "Epoch [394/500], Loss: 0.0030\n",
            "Epoch [395/500], Loss: 0.0038\n",
            "Epoch [396/500], Loss: 0.0034\n",
            "Epoch [397/500], Loss: 0.0028\n",
            "Epoch [398/500], Loss: 0.0046\n",
            "Epoch [399/500], Loss: 0.0027\n",
            "Epoch [400/500], Loss: 0.0011\n",
            "Epoch [401/500], Loss: 0.0046\n",
            "Epoch [402/500], Loss: 0.0099\n",
            "Epoch [403/500], Loss: 0.0036\n",
            "Epoch [404/500], Loss: 0.0048\n",
            "Epoch [405/500], Loss: 0.0034\n",
            "Epoch [406/500], Loss: 0.0010\n",
            "Epoch [407/500], Loss: 0.0046\n",
            "Epoch [408/500], Loss: 0.0033\n",
            "Epoch [409/500], Loss: 0.0026\n",
            "Epoch [410/500], Loss: 0.0012\n",
            "Epoch [411/500], Loss: 0.0114\n",
            "Epoch [412/500], Loss: 0.0068\n",
            "Epoch [413/500], Loss: 0.0079\n",
            "Epoch [414/500], Loss: 0.0051\n",
            "Epoch [415/500], Loss: 0.1996\n",
            "Epoch [416/500], Loss: 0.0023\n",
            "Epoch [417/500], Loss: 0.1645\n",
            "Epoch [418/500], Loss: 0.0009\n",
            "Epoch [419/500], Loss: 0.0027\n",
            "Epoch [420/500], Loss: 0.0039\n",
            "Epoch [421/500], Loss: 0.0028\n",
            "Epoch [422/500], Loss: 0.0029\n",
            "Epoch [423/500], Loss: 0.0034\n",
            "Epoch [424/500], Loss: 0.1815\n",
            "Epoch [425/500], Loss: 0.0022\n",
            "Epoch [426/500], Loss: 0.0014\n",
            "Epoch [427/500], Loss: 0.0034\n",
            "Epoch [428/500], Loss: 0.0020\n",
            "Epoch [429/500], Loss: 0.0019\n",
            "Epoch [430/500], Loss: 0.0033\n",
            "Epoch [431/500], Loss: 0.0031\n",
            "Epoch [432/500], Loss: 0.0019\n",
            "Epoch [433/500], Loss: 0.0075\n",
            "Epoch [434/500], Loss: 0.0039\n",
            "Epoch [435/500], Loss: 0.0017\n",
            "Epoch [436/500], Loss: 0.0035\n",
            "Epoch [437/500], Loss: 0.0007\n",
            "Epoch [438/500], Loss: 0.0050\n",
            "Epoch [439/500], Loss: 0.0013\n",
            "Epoch [440/500], Loss: 0.0031\n",
            "Epoch [441/500], Loss: 0.0012\n",
            "Epoch [442/500], Loss: 0.0039\n",
            "Epoch [443/500], Loss: 0.0020\n",
            "Epoch [444/500], Loss: 0.0013\n",
            "Epoch [445/500], Loss: 0.0034\n",
            "Epoch [446/500], Loss: 0.0034\n",
            "Epoch [447/500], Loss: 0.0010\n",
            "Epoch [448/500], Loss: 0.0002\n",
            "Epoch [449/500], Loss: 0.0038\n",
            "Epoch [450/500], Loss: 0.0019\n",
            "Epoch [451/500], Loss: 0.0004\n",
            "Epoch [452/500], Loss: 0.0017\n",
            "Epoch [453/500], Loss: 0.0022\n",
            "Epoch [454/500], Loss: 0.2082\n",
            "Epoch [455/500], Loss: 0.0038\n",
            "Epoch [456/500], Loss: 0.0040\n",
            "Epoch [457/500], Loss: 0.0031\n",
            "Epoch [458/500], Loss: 0.0018\n",
            "Epoch [459/500], Loss: 0.0032\n",
            "Epoch [460/500], Loss: 0.0020\n",
            "Epoch [461/500], Loss: 0.0016\n",
            "Epoch [462/500], Loss: 0.0021\n",
            "Epoch [463/500], Loss: 0.0026\n",
            "Epoch [464/500], Loss: 0.0011\n",
            "Epoch [465/500], Loss: 0.0033\n",
            "Epoch [466/500], Loss: 0.0026\n",
            "Epoch [467/500], Loss: 0.0011\n",
            "Epoch [468/500], Loss: 0.0015\n",
            "Epoch [469/500], Loss: 0.0009\n",
            "Epoch [470/500], Loss: 0.0011\n",
            "Epoch [471/500], Loss: 0.0032\n",
            "Epoch [472/500], Loss: 0.0007\n",
            "Epoch [473/500], Loss: 0.0010\n",
            "Epoch [474/500], Loss: 0.0004\n",
            "Epoch [475/500], Loss: 0.0014\n",
            "Epoch [476/500], Loss: 0.2045\n",
            "Epoch [477/500], Loss: 0.0025\n",
            "Epoch [478/500], Loss: 0.0027\n",
            "Epoch [479/500], Loss: 0.0011\n",
            "Epoch [480/500], Loss: 0.0021\n",
            "Epoch [481/500], Loss: 0.0026\n",
            "Epoch [482/500], Loss: 0.0022\n",
            "Epoch [483/500], Loss: 0.0004\n",
            "Epoch [484/500], Loss: 0.0022\n",
            "Epoch [485/500], Loss: 0.0006\n",
            "Epoch [486/500], Loss: 0.1777\n",
            "Epoch [487/500], Loss: 0.0004\n",
            "Epoch [488/500], Loss: 0.0018\n",
            "Epoch [489/500], Loss: 0.0020\n",
            "Epoch [490/500], Loss: 0.0031\n",
            "Epoch [491/500], Loss: 0.0019\n",
            "Epoch [492/500], Loss: 0.1954\n",
            "Epoch [493/500], Loss: 0.0012\n",
            "Epoch [494/500], Loss: 0.0057\n",
            "Epoch [495/500], Loss: 0.0017\n",
            "Epoch [496/500], Loss: 0.0029\n",
            "Epoch [497/500], Loss: 0.0004\n",
            "Epoch [498/500], Loss: 0.0017\n",
            "Epoch [499/500], Loss: 0.0001\n",
            "Epoch [500/500], Loss: 0.0018\n",
            "final loss: 0.0018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": input_size,\n",
        "\"hidden_size\": hidden_size,\n",
        "\"output_size\": output_size,\n",
        "\"all_words\": all_words,\n",
        "\"tags\": tags\n",
        "}\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr3rMKnxuG3h",
        "outputId": "46096181-c1f9-47ad-bd41-81634591ec7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)"
      ],
      "metadata": {
        "id": "wSigmZRDuH5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = data[\"input_size\"]\n",
        "hidden_size = data[\"hidden_size\"]\n",
        "output_size = data[\"output_size\"]\n",
        "all_words = data['all_words']\n",
        "tags = data['tags']\n",
        "model_state = data[\"model_state\"]\n"
      ],
      "metadata": {
        "id": "idtwfkGxuKtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNet(input_size, hidden_size, output_size).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjQ-AHykuNua",
        "outputId": "6805abd0-a0b2-4231-cea7-624ab8f099c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNet(\n",
              "  (l1): Linear(in_features=172, out_features=8, bias=True)\n",
              "  (l2): Linear(in_features=8, out_features=8, bias=True)\n",
              "  (l3): Linear(in_features=8, out_features=20, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"MediCall\"\n",
        "print(\"Let's chat! (type 'quit' to exit)\")\n",
        "while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "    sentence = input(\"You: \")\n",
        "    if sentence == \"quit\":\n",
        "        break\n",
        "\n",
        "    sentence = tokenize(sentence)\n",
        "    X = bag_of_words(sentence, all_words)\n",
        "    X = X.reshape(1, X.shape[0])\n",
        "    X = torch.from_numpy(X).to(device)\n",
        "\n",
        "    output = model(X)\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = tags[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.75:\n",
        "        \"\"\"for i in range(100):\n",
        "            if tag == intents[i][\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent[i]['Doctor'])}\")\"\"\"\n",
        "        for intent in intents2['intents']:\n",
        "            if tag == intent[\"tag\"]:\n",
        "                print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "    else:\n",
        "        print(f\"{bot_name}: Sorry! I do not understand...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W11GRquEuUN_",
        "outputId": "2a1425f1-dd2d-40ae-bb0a-39abb8d83098"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'quit' to exit)\n",
            "You: Hello\n",
            "MediCall: Hey :-)\n",
            "You: I feel very anxious\n",
            "MediCall: Sorry! I do not understand...\n",
            "You: I'm feeling anxious\n",
            "MediCall: You seem to be having a panic attack. Do not fight it. Stay where you are, if possible. Breathe slowly and deeply. Focus on positive, peaceful and relaxing images.\n",
            "You: I'm sweating\n",
            "MediCall: You seem to be having a panic attack. Do not fight it. Stay where you are, if possible. Breathe slowly and deeply. Focus on positive, peaceful and relaxing images.\n",
            "You: I feel a sharp chest pain\n",
            "MediCall: Chew and swallow an aspirin, unless you are allergic to aspirin or have been told by your doctor never to take aspirin. Cardiologists nearby : https://bit.ly/cardiologists_nearby , A hospital near you : http://pulsecardiologygroup.co.uk/sussex-cardiovascular-clinic/\n",
            "You: What to do for asharp chest pain?\n",
            "MediCall: Sorry! I do not understand...\n",
            "You: What to do for a sharp chest pain?\n",
            "MediCall: Sorry! I do not understand...\n",
            "You: How to treat a sharp chest pain and increased heart rate?\n",
            "MediCall: Chew and swallow an aspirin, unless you are allergic to aspirin or have been told by your doctor never to take aspirin. Cardiologists nearby : https://bit.ly/cardiologists_nearby , A hospital near you : http://pulsecardiologygroup.co.uk/sussex-cardiovascular-clinic/\n",
            "You: My vision is diminishing\n",
            "MediCall: Consult an opthalmologist nearby. Follow this link to book an appointment - https://bit.ly/opthalmologists_nearby\n",
            "You: My apetitite is decreasing\n",
            "MediCall: Sorry! I do not understand...\n",
            "You: my apetite is decreasing\n",
            "MediCall: Here's a joke for you - Why did the hipster burn his mouth? He drank the coffee before it was cool.\n",
            "You: I could not sleep last night because of breathlessness\n",
            "MediCall: Find best pulmonologists nearby. Book an appointment now - https://bit.ly/pulmonologists_nearby\n",
            "You: I have a very high fever\n",
            "MediCall: Have a paracetamol to reduce the temperature. Drink lots of water, keep yourself hydrated. Find the best general physicians nearby - https://bit.ly/pcp_nearby\n",
            "You: How to check if one is obsese?\n",
            "MediCall: Check your BMI here to know if you're really obese- https://patient.info/doctor/bmi-calculator-calculator. If you're obese or extremely obese, consider consulting a dietician nearby. Find dieticians/nutritionists nearby - https://bit.ly/diet_care\n",
            "You: Why did I experience a sudden weight loss ?\n",
            "MediCall: Eat healthy and organic foods. Practise yoga everyday. Include fruits and green vegetables in your diet. Consult a dietician for advice - https://bit.ly/diet_care \n",
            "You: How do I treat a joint pain?\n",
            "MediCall: For moderate-to-severe joint pain with swelling, an over-the-counter or prescription nonsteroidal anti-inflammatory drug (NSAID) such as aspirin, celecoxib, ibuprofen, or naproxen can provide relief. NSAIDs can have side effects, potentially increasing your risk for gastrointestinal bleeding.If you have mild pain without any swelling, acetaminophen can be effective. Be careful when taking this medicine though, especially if you drink alcohol, because high doses may cause liver damage. Because of the risks, you should take any of these pain medications with caution.\n",
            "You: How do I treat a tooth ache?\n",
            "MediCall: Salt water gargles will help soothe the pain. Book an appointment with a nearby dentist - https://bit.ly/dentists_visit\n",
            "You: Find doctors or hospitals asap\n",
            "MediCall: Websites to find nearest hospitals include : Zocdoc - https://www.zocdoc.com/ , Healthgrades - https://www.healthgrades.com/ , WebMD - https://www.webmd.com/ , Yelp - https://www.yelp.com/ , Google Maps - https://www.google.com/maps/\n",
            "You: Find diagnostic centres asap\n",
            "MediCall: Websites to find diagnostic centres nearby : BMI Healthcare - https://www.bmihealthcare.co.uk/ , Nuffield Health - https://www.nuffieldhealth.com/ , The London Clinic - https://www.thelondonclinic.co.uk/ , Spire Healthcare - https://www.spirehealthcare.com/ , Medicspot - https://www.medicspot.co.uk/\n",
            "You: thank you for your service\n",
            "MediCall: My pleasure\n",
            "You: goodbye\n",
            "MediCall: Have a nice day\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colabcode\n",
        "!pip install fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq3r4a2-CWkO",
        "outputId": "fb3c52bd-6b8d-496b-b75a-bb7f7a381bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colabcode\n",
            "  Downloading colabcode-0.3.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting nest-asyncio==1.4.3\n",
            "  Downloading nest_asyncio-1.4.3-py3-none-any.whl (5.3 kB)\n",
            "Collecting uvicorn==0.13.1\n",
            "  Downloading uvicorn-0.13.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok>=5.0.0\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jupyterlab==3.0.7\n",
            "  Downloading jupyterlab-3.0.7-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (2.11.3)\n",
            "Collecting jupyterlab-server~=2.0\n",
            "  Downloading jupyterlab_server-2.19.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (6.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (7.9.0)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading nbclassic-0.5.2-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (23.0)\n",
            "Collecting jupyter-server~=1.2\n",
            "  Downloading jupyter_server-1.23.6-py3-none-any.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.4/347.4 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from jupyterlab==3.0.7->colabcode) (5.2.0)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.8/dist-packages (from uvicorn==0.13.1->colabcode) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok>=5.0.0->colabcode) (6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.10->jupyterlab==3.0.7->colabcode) (2.0.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.7.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (23.2.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.8.0)\n",
            "Collecting nbconvert>=6.4.4\n",
            "  Downloading nbconvert-7.2.9-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 KB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.16.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.13.3)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.5.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (21.3.0)\n",
            "Collecting anyio<4,>=3.1.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (6.1.12)\n",
            "Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.8/dist-packages (from jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (5.7.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core->jupyterlab==3.0.7->colabcode) (3.0.0)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2.11.0)\n",
            "Collecting jinja2>=2.10\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.8/dist-packages (from jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (6.0.0)\n",
            "Collecting jsonschema>=4.17.3\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting json5>=0.9.0\n",
            "  Downloading json5-0.9.11-py2.py3-none-any.whl (19 kB)\n",
            "Collecting requests>=2.28\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting notebook-shim>=0.1.0\n",
            "  Downloading notebook_shim-0.2.2-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.8/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (5.3.4)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.8/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Collecting nbclassic~=0.2\n",
            "  Downloading nbclassic-0.5.1-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.8-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.7-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.6-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.5-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.4-py3-none-any.whl (9.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.2-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.4.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading nbclassic-0.3.7-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: notebook<7 in /usr/local/lib/python3.8/dist-packages (from nbclassic~=0.2->jupyterlab==3.0.7->colabcode) (6.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->jupyterlab==3.0.7->colabcode) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.10)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.8/dist-packages (from babel>=2.10->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2022.7.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.3->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.13.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->jupyterlab==3.0.7->colabcode) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (0.19.3)\n",
            "Collecting pkgutil-resolve-name>=1.3.10\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=4.17.3->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (5.10.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from jupyter-client>=6.1.12->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.8.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (6.0.0)\n",
            "Collecting jupyterlab-pygments\n",
            "  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.5.0)\n",
            "Collecting nbclient>=0.5.0\n",
            "  Downloading nbclient-0.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mistune<3,>=2.0.3\n",
            "  Downloading mistune-2.0.5-py2.py3-none-any.whl (24 kB)\n",
            "Collecting tinycss2\n",
            "  Downloading tinycss2-1.2.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.16.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.0.7->colabcode) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.0.7->colabcode) (0.2.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (3.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28->jupyterlab-server~=2.0->jupyterlab==3.0.7->colabcode) (2022.12.7)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.3->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.8/dist-packages (from argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (21.2.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (1.15.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.2->jupyterlab==3.0.7->colabcode) (2.21)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19792 sha256=8e4f2f0299f12dfa841a7bb233348c09b5500005f3e1ed89cfed7979a526e523\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f2/70/526da675d32f17577ec47ac4c663084efe39d47c826b6c3bb1\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: mistune, json5, websocket-client, tinycss2, sniffio, requests, pyngrok, pkgutil-resolve-name, nest-asyncio, jupyterlab-pygments, jinja2, jedi, h11, uvicorn, jsonschema, anyio, nbclient, nbconvert, jupyter-server, notebook-shim, jupyterlab-server, nbclassic, jupyterlab, colabcode\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 2.11.3\n",
            "    Uninstalling Jinja2-2.11.3:\n",
            "      Successfully uninstalled Jinja2-2.11.3\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.3.3\n",
            "    Uninstalling jsonschema-4.3.3:\n",
            "      Successfully uninstalled jsonschema-4.3.3\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 5.6.1\n",
            "    Uninstalling nbconvert-5.6.1:\n",
            "      Successfully uninstalled nbconvert-5.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-3.6.2 colabcode-0.3.0 h11-0.14.0 jedi-0.18.2 jinja2-3.1.2 json5-0.9.11 jsonschema-4.17.3 jupyter-server-1.23.6 jupyterlab-3.0.7 jupyterlab-pygments-0.2.2 jupyterlab-server-2.19.0 mistune-2.0.5 nbclassic-0.3.7 nbclient-0.7.2 nbconvert-7.2.9 nest-asyncio-1.4.3 notebook-shim-0.2.2 pkgutil-resolve-name-1.3.10 pyngrok-5.2.1 requests-2.28.2 sniffio-1.3.0 tinycss2-1.2.1 uvicorn-0.13.1 websocket-client-1.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi) (1.10.4)\n",
            "Collecting starlette<0.26.0,>=0.25.0\n",
            "  Downloading starlette-0.25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette<0.26.0,>=0.25.0->fastapi) (3.6.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (2.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (1.3.0)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.92.0 starlette-0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI"
      ],
      "metadata": {
        "id": "5EK4gxkEvAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcoyxwguCpRE",
        "outputId": "39043001-2155-4bdb-93ab-594ac47a98de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (0.92.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (0.13.1)\n",
            "Requirement already satisfied: starlette<0.26.0,>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from fastapi) (0.25.0)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi) (1.10.4)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: click==7.* in /usr/local/lib/python3.8/dist-packages (from uvicorn) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette<0.26.0,>=0.25.0->fastapi) (3.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80ifrVtxDFts",
        "outputId": "2f8f5ea5-98e9-4f7c-9990-f5f8d45e4dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.8/dist-packages (0.92.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.8/dist-packages (1.5.6)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.8/dist-packages (5.2.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.8/dist-packages (0.20.0)\n",
            "Requirement already satisfied: starlette<0.26.0,>=0.25.0 in /usr/local/lib/python3.8/dist-packages (from fastapi) (0.25.0)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.8/dist-packages (from fastapi) (1.10.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from pyngrok) (6.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette<0.26.0,>=0.25.0->fastapi) (3.6.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.26.0,>=0.25.0->fastapi) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=['*'],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=['*'],\n",
        "    allow_headers=['*'],\n",
        ")\n",
        "\n",
        "@app.get('/')\n",
        "async def root():\n",
        "    return {\"Hello\":\"World\"}"
      ],
      "metadata": {
        "id": "aUqunlIYvihh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-vImU2PFWyR",
        "outputId": "30465026-610a-4976-c7fa-4f260f54872e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://bb9a-34-67-87-73.ngrok.io\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [643]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     109.123.102.140:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     109.123.102.140:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class request_body(BaseModel):\n",
        "  sentence : str\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "sH4foIlZF4XA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0J-p8RK4ULCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"@app.post('/predict')\n",
        "def predict(data : request_body):\n",
        "\n",
        "    #class_idx = clf.predict(test_data)[0]\n",
        "    #return { 'class' : iris.target_names[class_idx]}\n",
        "    bot_name = \"MediCall\"\n",
        "    print(\"Let's chat! (type 'quit' to exit)\")\n",
        "    while True:\n",
        "    # sentence = \"do you use credit cards?\"\n",
        "        sentence = [[data.sentence]]\n",
        "        if sentence[0][0] == \"quit\":\n",
        "            break\n",
        "\n",
        "        sentence = tokenize(sentence[0][0])\n",
        "        X = bag_of_words(sentence, all_words)\n",
        "        X = X.reshape(1, X.shape[0])\n",
        "        X = torch.from_numpy(X).to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "        tag = tags[predicted.item()]\n",
        "\n",
        "        probs = torch.softmax(output, dim=1)\n",
        "        prob = probs[0][predicted.item()]\n",
        "        if prob.item() > 0.75:\n",
        "            for i in range(100):\n",
        "                if tag == intents[i][\"tag\"]:\n",
        "                    print(f\"{bot_name}: {random.choice(intent[i]['Doctor'])}\")\n",
        "            for intent in intents2['intents']:\n",
        "                if tag == intent[\"tag\"]:\n",
        "                    return {\"MediCall\" : random.choice(intent['responses'])}\n",
        "        else:\n",
        "            return {\"MediCall\" : \"Sorry I don't understand.\"}\n",
        "            \"\"\""
      ],
      "metadata": {
        "id": "V8hX4_hGI_qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6yNt-mYPKpwO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}